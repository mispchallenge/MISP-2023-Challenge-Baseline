model:
  network_name: mease
  network_setting:
    out_channels: 201
    skip_convert: False
    input_types: [ mixture_wave, lip_frames]
    extractor_types: [ lps, multimodal_embedding ]
    extractor_settings:
      - n_fft: 400
        hop_length: 160
        win_type: hamming
        win_length: 400
        cmvn: ./cmvn/cmvn_lps_misp_sim_trainset.pt
        requires_grad: False
      - extract_feature: True
        skip_convert: True
        input_types: [ mixture_wave, lip_frames ]
        extractor_types: [ fbank, gray_crop_flip ]
        extractor_settings:
          - n_fft: 400
            hop_length: 160
            win_type: hamming
            win_length: 400
            cmvn: ./cmvn/cmvn_fbank_htk_misp_sim_trainset.pt
            f_min: 0
            f_max: 8000
            n_mels: 40
            sample_rate: 16000
            norm: slaney
            preemphasis_coefficient: 0.97
            vtln: False
            vtln_low: 0
            vtln_high: 8000
            vtln_warp_factor: 1.
          - channel_input: bgr
            size: [ 88, 88 ]
            random: False
        frontend_types: [ conv1d, conv3d ]
        frontend_settings:
          - out_channels: 64
            conv1d_kernel: 1
            conv1d_stride: 1
            act_type: prelu
          - out_channels: 64
            conv3d_kernel: [ 5, 7, 7 ]
            conv3d_stride: [ 1, 2, 2 ]
            act_type: prelu
            pool3d_kernel: [ 1, 3, 3 ]
            pool3d_stride: [ 1, 2, 2 ]
        backbone_types: [ resnet1d, resnet2d ]
        backbone_settings:
          - block_type: basic1d
            block_num: 2
            act_type: prelu
            hidden_channels: [ 64, 128, 256, 512 ]
            stride: [ 1, 1, 1, 1 ]
            expansion: 1
            downsample_type: avgpool
          - block_type: basic2d
            block_num: 2
            act_type: prelu
            hidden_channels: [ 64, 128, 256, 512 ]
            stride: [ 1, 2, 2, 2 ]
            expansion: 1
            downsample_type: avgpool
        fuse_type: tcn
        fuse_setting:
          hidden_channels: [ 768, 768, 768, 768 ]
          kernels_size: [ 3, 5, 7 ]
          dropout: 0.2
          act_type: prelu
          dwpw: False
          downsample_type: norm
        requires_grad: False
    encoder_types: [ DSResConv, DSResConv ]
    encoder_settings:
      - layer_num: 5
        out_channels: 1536
        kernel: 5
        stride: [ 1, 1, 1, 1, 1 ]
        dilation: 1
        norm_type: BN1d
        requires_grad: True
      - layer_num: 10
        out_channels: 1536
        kernel: 5
        stride: [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ]
        dilation: 1
        norm_type: BN1d
        requires_grad: True
    fusion_type: cat
    fusion_setting: { }
    decoder_type: DSResConv
    decoder_setting:
      layer_num: 15
      out_channels: 1536
      kernel: 5
      requires_grad: True
      stride: [ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ]
      dilation: 1
      norm_type: BN1d
  pretrained_num: 1
  pretrained_model:
    - - ./pretrainModels/mee.tar
      - model_params
  replace_keys:
    - audio_extractor.extractor.: multimodal_embedding_extractor.embedding_extractor.audio_extractor.extractor.
      audio_frontend.frontend.: multimodal_embedding_extractor.embedding_extractor.audio_frontend.frontend.
      audio_backbone.backbone.: multimodal_embedding_extractor.embedding_extractor.audio_backbone.backbone.
      visual_frontend.frontend.: multimodal_embedding_extractor.embedding_extractor.visual_frontend.frontend.
      visual_backbone.backbone.: multimodal_embedding_extractor.embedding_extractor.visual_backbone.backbone.
      audio_visual_fusion.fusion.: multimodal_embedding_extractor.embedding_extractor.audio_visual_fusion.fusion.
  fixed_type: fixed_keys
  fixed_keys:
    - multimodal_embedding_extractor.embedding_extractor.


data:
  loader_output:
    items: &input_items [ mixture_wave, lip_frames ]  #[ mixture_wave, lip_frames, irm ]
    shapes: &input_shapes [ [ 0 ], [ 0, 96, 96, 3 ]]
    pad_values: &input_pad_values [ 0, 0 ]
  model_input:
    items: {'mixture_wave': 0, 'lip_frames': 2, 'length': 1}
    gpu_items: [mixture_wave, lip_frames]
  model_output:
    items: &output_items [predicted_mask]
    store_items: {'predicted_mask': 0}
    store_item2length: {'predicted_mask': 1}
  loss_input:
    gpu_items:
    - clean_wave
    - mixture_wave
    items_from_loader:
      mixture_wave: 0
      clean_wave: 4
      length: 1
    items_from_model:
      net_output: 0
  train:
    annotate:
      - ../data_prepare/misp_sim_trainset.json
    repeat: 1
    max_duration: 100
    hop_duration: 6
    items: *input_items
    dynamic: True
    batch_size: 16
    max_batch_size: 256
    bucket_length_multiplier: 1.2
    shuffle: True
    drop_last: False
    num_workers: 4
    pin_memory: False
    target_shape: *input_shapes
    pad_value: *input_pad_values
  test:
    annotate:
      - ../data_prepare/misp_real_devset.json
    repeat: 1
    max_duration: 100
    hop_duration: 6
    items: *input_items
    dynamic: True
    batch_size: 16
    max_batch_size: 256
    bucket_length_multiplier: 1.2
    shuffle: True
    drop_last: False
    num_workers: 4
    pin_memory: False
    target_shape: *input_shapes
    pad_value: *input_pad_values

optimizer:
  optimizer_type: Adam
  group_num: 1
  force_lr: False
  common_setting:
    lr: 0.0001
    betas: [0.9, 0.999]
    eps: 0.00000001
    weight_decay: 0.0
    amsgrad: False

scheduler:
  scheduler_type: improve
  scheduler_setting:
    factor: 0.5
    patience: 3

loss:
  loss_setting:
    extractor_setting:
      hop_length: 160
      mask_type: irm
      n_fft: 400
      win_length: 400
      win_type: hamming
    mode: error
  loss_type: misp_mse_calcIRMLabel

gradient:
  grad_clip: 5.

reconstruct:
  predict_items: *output_items
  data_items: [ mixture_wave ]
  reconstruct_items: &reconstruct_items [ reconstruct_wave ]
  reconstruct_type: mack2wave
  reconstruct_settings:
    n_fft: 400
    hop_length: 160
    win_type: hamming
    win_length: 400
    rescale: False
